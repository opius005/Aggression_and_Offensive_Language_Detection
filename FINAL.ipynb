{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification,AutoConfig,DataCollatorWithPadding,AdamW,get_scheduler\n",
    "from transformers.utils import ModelOutput\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset,DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import Optional, Tuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(46)\n",
    "# np.random.seed(46)\n",
    "# torch.manual_seed(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "class Hyper_params:\n",
    "    def __init__(self):\n",
    "        self.max_length=128\n",
    "        self.batch_size=8\n",
    "        self.learning_rate=0.00002\n",
    "        self.num_epochs=3\n",
    "        self.aggression_n_hidden=2\n",
    "        self.offense_n_hidden=3\n",
    "        self.dropout_prob=0.15\n",
    "        self.w_a=0.4\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hypams=Hyper_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Custom_Model_ouputs(ModelOutput):\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    loss_a: Optional[torch.FloatTensor] = None\n",
    "    loss_o: Optional[torch.FloatTensor] = None\n",
    "    logits_a: torch.FloatTensor = None\n",
    "    logits_o: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name=\"l3cube-pune/hing-roberta\"\n",
    "# model_name='FacebookAI/xlm-roberta-base'\n",
    "# model_name='FacebookAI/roberta-base'\n",
    "model_name='google-bert/bert-base-uncased'\n",
    "\n",
    "# train_path=\"/home/bharat/bharath/LBP/original_split/train.csv\"\n",
    "# test_path=\"/home/bharat/bharath/LBP/original_split/test.csv\"\n",
    "# val_path=\"/home/bharat/bharath/LBP/original_split/validation.csv\"\n",
    "\n",
    "\n",
    "# train_path=\"/home/bharat/bharath/LBP/same_split/cs_train.csv\"\n",
    "# test_path=\"/home/bharat/bharath/LBP/same_split/cs_test.csv\"\n",
    "# val_path=\"/home/bharat/bharath/LBP/same_split/cs_validation.csv\"\n",
    "\n",
    "train_path=\"/home/bharat/bharath/LBP/same_split/ms_train.csv\"\n",
    "test_path=\"/home/bharat/bharath/LBP/same_split/ms_test.csv\"\n",
    "val_path=\"/home/bharat/bharath/LBP/same_split/ms_validation.csv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(train_path,encoding='utf-8')\n",
    "test_set=pd.read_csv(test_path,encoding='utf-8')\n",
    "val_set=pd.read_csv(val_path,encoding='utf-8')\n",
    "\n",
    "# train_set=train_set.head(10)\n",
    "# test_set=test_set.head(10)\n",
    "# val_set=val_set.head(10)\n",
    "print(train_set.shape,test_set.shape,val_set.shape)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=Dataset.from_pandas(train_set[['tweet','aggression','offense']])\n",
    "test_ds=Dataset.from_pandas(test_set[['tweet','aggression','offense']])\n",
    "val_ds=Dataset.from_pandas(val_set[['tweet','aggression','offense']])\n",
    "data =DatasetDict({\n",
    "    'train' : train_ds,\n",
    "    'val': val_ds,\n",
    "    'test' : test_ds\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['tweet'],truncation=True,max_length=hypams.max_length,padding='max_length',return_tensors='pt')\n",
    "\n",
    "tokenized_ds=data.map(tokenize,batched=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds.set_format('torch',columns=['input_ids','attention_mask','aggression','offense'])\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self,num_labels,inp_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(inp_size, 768)\n",
    "        self.dropout = nn.Dropout(hypams.dropout_prob)\n",
    "        self.out_proj = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]  \n",
    "        # x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        # x = torch.tanh(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Compute class weights\n",
    "        class_counts = torch.bincount(target, minlength=self.num_classes).float()\n",
    "        total_samples = class_counts.sum()\n",
    "        class_weights = total_samples / (self.num_classes * class_counts)\n",
    "\n",
    "        # Compute cross-entropy loss with class weights\n",
    "        ce_loss = F.cross_entropy(input, target, weight=class_weights)\n",
    "        return ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class task_specific_hidden_layers(nn.Module):\n",
    "    def __init__ (self,n_hidden,hin_size=768,hout_size=768):\n",
    "        super(task_specific_hidden_layers,self).__init__()\n",
    "        self.dropout=nn.Dropout(hypams.dropout_prob)\n",
    "        self.relu=nn.LayerNorm(hin_size)\n",
    "        self.hidden=nn.ModuleList()\n",
    "        for k in range(0,n_hidden):\n",
    "            self.hidden.append(\n",
    "                nn.Linear(hin_size,hout_size)\n",
    "            )\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.hidden[:-1]:\n",
    "            input=layer(input)\n",
    "            input=self.relu(input)\n",
    "            input=self.dropout(input)\n",
    "\n",
    "        return self.hidden[-1](input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hingroberta_MTL (nn.Module):\n",
    "\n",
    "    def __init__(self,n_a_labels=3,n_o_labels=2):\n",
    "\n",
    "        super(Hingroberta_MTL,self).__init__()\n",
    "        self.device=torch.device('cuda')\n",
    "        self.n_a_labels=n_a_labels\n",
    "        self.n_o_labels=n_o_labels\n",
    "        self.model=AutoModelForSequenceClassification.from_pretrained(model_name,config=AutoConfig.from_pretrained(model_name,\n",
    "                                                                                                            # num_labels=5,\n",
    "                                                                                                            # problem_type=\"multi_label_classification\",\n",
    "                                                                                                            hidden_dropout_prob=hypams.dropout_prob,\n",
    "                                                                                                            output_attentions=True,\n",
    "                                                                                                            output_hidden_states=True,\n",
    "                                                                                                            # return_dict=False\n",
    "                                                                                                            ))\n",
    "        self.classifier_a=ClassificationHead(n_a_labels,768)\n",
    "        self.classifier_o=ClassificationHead(n_o_labels,768)\n",
    "        self.aggression_task=task_specific_hidden_layers(n_hidden=hypams.aggression_n_hidden)\n",
    "        self.offensize_task=task_specific_hidden_layers(n_hidden=hypams.offense_n_hidden)\n",
    "        \n",
    "\n",
    "    def forward(self,input_ids=None,attention_mask=None,aggression=None,offense=None):\n",
    "        outputs=self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sequence_output=outputs.hidden_states[-1]\n",
    "        # sequence_output_a=self.aggression_task(sequence_output)\n",
    "        sequence_output_o=self.offensize_task(sequence_output)\n",
    "        logits_a=self.classifier_a(sequence_output)\n",
    "        logits_o=self.classifier_o(sequence_output_o+sequence_output)\n",
    "        loss=None\n",
    "        if aggression is not None and offense is not None:\n",
    "\n",
    "            loss_fct_a=WeightedCrossEntropyLoss(self.n_a_labels)\n",
    "            loss_fct_o=WeightedCrossEntropyLoss(self.n_o_labels)\n",
    "            loss_a=loss_fct_a(logits_a.view(-1,self.n_a_labels),aggression.view(-1))\n",
    "            loss_o=loss_fct_o(logits_o.view(-1,self.n_o_labels),offense.view(-1))\n",
    "            \n",
    "            loss=(hypams.w_a)*loss_a+(1-hypams.w_a)*loss_o\n",
    "\n",
    "        return Custom_Model_ouputs(\n",
    "            loss=loss,\n",
    "            loss_a=loss_a,\n",
    "            loss_o=loss_o,\n",
    "            logits_a=logits_a,\n",
    "            logits_o=logits_o,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Hingroberta_MTL().to(hypams.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(\n",
    "    tokenized_ds['train'],shuffle=True,batch_size=hypams.batch_size,collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader=DataLoader(\n",
    "    tokenized_ds['val'],batch_size=hypams.batch_size,collate_fn=data_collator\n",
    ")\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=AdamW(model.parameters(),lr=hypams.learning_rate,weight_decay=0.5)\n",
    "\n",
    "num_training_steps=hypams.num_epochs*len(train_dataloader)\n",
    "lr_scheduler=get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "progress_bar_train =tqdm(range(num_training_steps))\n",
    "progress_bar_eval=tqdm(range(hypams.num_epochs*len(eval_dataloader)))\n",
    "\n",
    "for epoch in range(hypams.num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(hypams.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        xyz=outputs\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss=[]\n",
    "    eval_loss_a=[]\n",
    "    eval_loss_o=[]\n",
    "    all_preds_a = []\n",
    "    all_labels_a = []\n",
    "    all_preds_o = []\n",
    "    all_labels_o = []\n",
    "    all_labels_o = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(hypams.device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        eval_loss.append(outputs.loss.cpu().item())\n",
    "        eval_loss_a.append(outputs.loss_a.cpu().item())\n",
    "        eval_loss_o.append(outputs.loss_o.cpu().item())\n",
    "        logits_a = outputs.logits_a\n",
    "        logits_o = outputs.logits_o\n",
    "        pred_a = torch.argmax(logits_a, dim=-1)\n",
    "        pred_o = torch.argmax(logits_o, dim=-1)\n",
    "        # Collecting predictions and labels for each batch\n",
    "        all_preds_a.extend(pred_a.cpu().numpy())\n",
    "        all_labels_a.extend(batch['aggression'].cpu().numpy())\n",
    "        all_preds_o.extend(pred_o.cpu().numpy())\n",
    "        all_labels_o.extend(batch['offense'].cpu().numpy())\n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "\n",
    "    print(classification_report(all_labels_a, all_preds_a))\n",
    "    print(classification_report(all_labels_o, all_preds_o))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_dataloader=DataLoader(\n",
    "    tokenized_ds['test'],batch_size=hypams.batch_size,collate_fn=data_collator\n",
    ")\n",
    "progress_bar_test=tqdm(range(len(test_dataloader)))\n",
    "\n",
    "all_preds_a = []\n",
    "all_labels_a = []\n",
    "all_preds_o = []\n",
    "all_labels_o = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(hypams.device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits_a = outputs.logits_a\n",
    "    logits_o = outputs.logits_o\n",
    "    pred_a = torch.argmax(logits_a, dim=-1)\n",
    "    pred_o = torch.argmax(logits_o, dim=-1)\n",
    "    # Collecting predictions and labels for each batch\n",
    "    all_preds_a.extend(pred_a.cpu().numpy())\n",
    "    all_labels_a.extend(batch['aggression'].cpu().numpy())\n",
    "    all_preds_o.extend(pred_o.cpu().numpy())\n",
    "    all_labels_o.extend(batch['offense'].cpu().numpy())\n",
    "    progress_bar_test.update(1)\n",
    "\n",
    "\n",
    "print(classification_report(all_labels_a, all_preds_a))\n",
    "print(classification_report(all_labels_o, all_preds_o))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
